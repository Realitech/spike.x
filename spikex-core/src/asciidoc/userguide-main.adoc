:icons: font
:iconfont-cdn: https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css

= Spike.x User Guide
Christoffer Lindevall
February v1.0, 2016: Dawn of the Four-toed hedgehog
:keywords: documentation, spikex, vertx, vert.x

:numbered!:
[abstract]
= Sense. React. Visualize.

Reactive event monitoring and data analysis built on top of http://vertx.io/vertx2[Vert.x].

Spike.x provides components for resource monitoring, for data filtering and streaming, 
for sending of notifications and for storing of metrics and events in various backends.

Spike.x can be used out-of-the-box for the following use cases:

icon:bar-chart[] Filtering, streaming and analysis of events::
* Tail logs and send events to https://www.elastic.co[Elasticsearch] or https://influxdata.com[InfluxDB]
* Collect JVM, CPU, network, filesystem and memory metrics
* Collect database metrics using custom SQL
* Query, analyze and present the data with https://www.elastic.co/products/kibana[Kibana] or http://grafana.org[Grafana]

icon:bell[] Event monitoring and sending of notifications::
* Monitor key data parameters 
* Trigger events based on custom rules
* Log and send notifications to interested parties
* Rule based sending of alarms

CAUTION: Spike.x 0.9.2 is still in its infancy. It will take some time before it can be 
considered production ready.

TIP: We are planning to migrate to http://vertx.io[Vert.x 3] as soon as possible.

:numbered:

== Introduction

Spike.x is built on top of the excellent http://vertx.io/vertx2[Vert.x platform]. Vert.x 
is one of the many modern actor-like application platforms for the JVM. 

.What is Vert.x?
****
"Vert.x is a polyglot, non-blocking, event-driven application platform that runs on the JVM."
-- http://vertx.io/vertx2/manual.html#what-is-vertx
****

You do not have to worry about thread synchronization when building applications 
on top of Vert.x. You communicate using messages and you compose your code
into one or more modules. The verticle is the main actor in Vert.x. These and many 
other concepts are explained in the http://vertx.io/vertx2/manual.html[Vert.x documentation].

Spike.x brings a few concepts of its own to the table. We introduce the following
concepts: activator, filter and command.

An activator is simply a verticle that is responsible for deploying and undeploying
verticles within a module. The activator is the main verticle of a module.

A filter is a verticle that receives input, sends output or does both. Spike.x 
comes with many filters that can be chained together in various ways. Here's a list 
of some of the built-in filters:

* Tail - reads lines from a log
* Mutate - modifies an event
* Limit - performs event limiting
* Grok - matches regexps against an event field
* HttpServer - receives events via HTTP (supports collectd and Nagios NRDP)
* Batch - creates a batch of events before publishing
* NSQ - publishes or subscribes events to/from NSQ
* InfluxDB - stores events in InfluxDB 0.10.x or later
* Elasticsearh - stores events in Elasticsearch

Commands are used to control the behaviour of verticles. We send commands to
activators in order to deploy and undeploy filters. These commands are an internal
detail of Spike.x. 

Spike.x has a Main class that is responsible for bootstrapping and starting 
the Vert.x platform. It also takes care of daemonizing Spike.x if needed on 
platforms that support this. Please see the spikex startup script for details.

When you start Spike.x it tries to load any required modules from a local deploy directory.

Spike.x depends on many well-established open source libraries. The following is a list 
of some of the core dependencies:

* SLF4J - Simple Logging Facade for Java
* Logback - SLF4J implementation
* Guava - Google's core libraries
* Joda-Time - Java date and time API (also part of Java 8)
* Bouncy Castle Crypto APIs for Java
* Hazelcast - Open Source In-Memory Data Grid (part of Vert.x)
* LZ4 Java - LZ4 compression and xxhash hashing for Java
* SnakeYAML - YAML parser and emitter for Java
* GS Collections - A supplement or replacement for the Java Collections Framework

=== Supported platforms

[TIP]
Spike.x can be installed on almost any platform that supports JDK 1.8 or newer to run. Please note that some of the filters only work on a specific platform.

The following platforms are currently supported:

* Windows 64-bit (Windows 7, Windows 2008 Server, etc.)
* Linux 64-bit (Ubuntu, CentOs, RedHat, etc.)
* Apple OS X 64-bit (Yosemite or newer)
* FreeBSD 64-bit (FreeBSD 10 and newer)

=== Quick start

You can grab the latest Spike.x installation package from https://bintray.com/spikex/generic/installer/view[Bintray].

Simply launch the installer and follow the instructions.

IMPORTANT: The installer requires Java to function and Spike.x requires JDK 1.8 or newer.

=== Directories and files

Spike.x is installed by default in `/var/lib/spikex` or in `C:\Program Files\Spike.x` on Windows. 

The main configuration file is called `spikex.yaml` and it can be found in the `conf` directory. 
You rarely need to modify this http://yaml.org[YAML] file. It lists the modules that you want to 
load from the `deploy` directory.

IMPORTANT: Be careful not to save broken configuration files since Spike.x reloads saved configuration files automatically.

Filters are configured using one or more filter configuration files. 
These http://yaml.org[YAML] files must start with the `filters` prefix and they are also stored in the `conf` directory.

Persistent data files are stored in the `data` directory. Whereas temporary data files, 
that can be removed after Spike.x has been stopped, are storde in the `tmp` directory.

.Spike.x directories within the home directory
[width="100%",options="header"]
|====================================================
|Directory         |Description
|bin               |Executables and service files
|conf              |Configuration and example files
|data              |Persistent data files
|deploy            |Deployable modules
|docs              |Javadocs and user guide
|lib               |Libraries required by Spike.x
|log               |Log files
|tmp               |Temprary data files
|====================================================

=== Wiki and issues

We maintain a small https://github.com/clidev/spike.x/wiki[Wiki] that you might find useful. 
The latest issues can be found in https://github.com/clidev/spike.x/issues[GitHub].

=== License

Spike.x is provided under the terms of the http://www.apache.org/licenses/LICENSE-2.0[Apache License, Version 2.0].

We carefully try to select and include only http://www.apache.org/licenses/LICENSE-2.0[Apache License, Version 2.0] 
compliant software with Spike.x.

== Installation

Spike.x runs on the https://en.wikipedia.org/wiki/Java_virtual_machine[Java VM]. It requires an adequate amount of memory and CPU resources to run. 
In a typical small scale scenario you would install Spike.x on a dedicated host that is not running a mission critical system. 
Spike.x would receive data from light-weight agents like https://collectd.org[collectd] and https://www.nsclient.org[NSClient++]. 
It would then "sanitize" the data and finally send it off to a backend.

The minimum system requirements usually depend on the amount of data being processed. 
Typically one CPU and 1 GB of memory should be adequate for most scenarios. The default installation uses the following JVM heap memory settings: 

* -Xms64m
* -Xmx256m

We would advise you to start with the defaults settings and monitor the resource usage of Spike.x using the <<module-metrics,Metrics>> module.



The latest version of Spike.x is found at https://bintray.com/spikex/generic/installer/view[Bintray]. The `EXE` installer is for Windows platforms, whereas the `JAR` installer is for all other platforms.
Please note that Spike.x requires JDK 1.8 or newer. We test Spike.x on the http://www.oracle.com/technetwork/java/javase/downloads/index.html[Oracle JDK] and the http://openjdk.java.net[OpenJDK]. See the platform instructions below for installation details.

=== Deployment models

InfluxDB and Grafana are used for storage and visualization in the deployment diagrams below but you could also use 
<<use-case-elasticsearch-kibana,Elasticsearch and Kibana>>.

.Simple deployment diagram
image:spikex-deployment-simple.png[]

This deployment could be used if you are testing some software and want to do a minimal installation. 
Nothing prevents you from installing the whole stack in one host if there's enough resources.

.Small-scale deployment diagram
image:spikex-deployment-small-scale.png[]

This deployment could be used for a production site where you want to keep the monitored data locally on private servers 
and you are not too concerned with scaling out the system.

.Cloud-based deployment diagram
image:spikex-deployment-cloud-based.png[]

This deployment is ideal when you have multiple sites to monitor and you want to centralize the storage and visualization. 
At the time of writing there is at least one https://customers.influxdb.com[hosted InfluxDB cloud service] that is supported by Spike.x. 
Any service that exposes the InfluxDB HTTP API should work with Spike.x. The setup for this deployment is explained in <<use-case-influxdb-grafana,the monitoring use case>>.

=== Spike.x on Linux, OS X and FreeBSD

Download the latest Spike.x `JAR` installer from https://bintray.com/spikex/generic/installer/view[Bintray].

[source,shell,subs="attributes"]
wget https://bintray.com/artifact/download/spikex/generic/spikex-{project-version}-installer.jar

[source,shell,subs="attributes"]
curl -O https://bintray.com/artifact/download/spikex/generic/spikex-{project-version}-installer.jar

Run the installer and follow the instructions.

[source,shell,subs="attributes"]
sudo java -jar spikex-{project-version}-installer.jar

Test that you are able to start Spike.x in the terminal. Spike.x should output that it has deployed all the standard modules successfully.

[source,shell]
cd /var/lib/spikex
su -c 'bin/spikex' spikex

==== Linux

Copy the appropriate service script to `/etc/init.d` or `/etc/systemd/system` depending on your Linux distribution. 
Please refer to the documentation of your distribution.

Start the Spike.x service and verify from the Spike.x log that the service started up without problems. 

.CentOS 7 example
[source,shell]
sudo cp bin/spikex.service /etc/systemd/system
sudo systemctl daemon-reload
sudo systemctl enable spikex
sudo systemctl start spikex
sudo less /var/lib/spikex/log/spikex.log

Stop the Spike.x service and create your filter configuration in `/var/lib/spikex/conf`. 
You can simply copy an example configuration from `/var/lib/spikex/conf/examples` and edit it to suit your needs.

Please see the <<Simple monitoring example>> to get started. Configuration details can be found in the <<Filters,filters section>>.

Remember to start the Spike.x service again once you have configured your filters and chains.

=== Spike.x on Windows

Download the latest Spike.x `EXE` installer from https://bintray.com/spikex/generic/installer/view[Bintray].

Launch the installer and follow the instructions.

.Welcome screen
image:spikex-win-install-welcome.png[]

.License agreement
image:spikex-win-install-license.png[]

.Target path
image:spikex-win-install-target-path.png[]

.Installation packages
image:spikex-win-install-packages.png[]

.Summary
image:spikex-win-install-summary.png[]

.Installation progress
image:spikex-win-install-progress.png[]

.Setup shortcuts
image:spikex-win-install-shortcuts.png[]

.Installation finished
image:spikex-win-install-finished.png[]

Verify that the Spike.x was installed successfully as a Windows service.

.Spike.x service
image:spikex-win-service.png[]

Stop the Spike.x service and create your filter configuration in `C:\Program Files\Spike.x\conf`.
You can simply copy an example configuration from `C:\Program Files\Spike.x\conf\examples` and edit it to suit your needs.

Please see the <<Simple monitoring example>> to get started. Configuration details can be found in the <<Filters,filters section>>.

Remember to start the Spike.x service again once you have configured your filters and chains.

=== Simple monitoring example

We start by defining an input filter that sends its data to the events log file. It also sends its output to an address called `metrics.spikex`. 

Use your favorite text editor and create a file called `filters-metrics-oshi.yaml` in the `conf` directory.

TIP: the entire `filters-metrics-oshi.yaml` file can be found in the `examples` directory.

.filters-metrics-oshi.yaml
[source]
----
modules: [
    { 
      module: 'io.spikex~spikex-filter',
      filters: [ 
            { alias: 'Metrics', verticle: 'io.spikex.filter.input.Metrics' },
            { alias: 'Log.out', verticle: 'io.spikex.filter.output.Logback' }
        ]
    }
]

chains: [
    {
        chain: 'system-metrics-load',
        filters: [
            {
                filter: 'Metrics',
                config: {
                    update-interval: '60s',
                    metric-selector: 'system.load',
                    add-tags: [ 'metric', 'spikex-metric-jvm', 'oshi' ]
                }
            },
            { '%OutputAddress': 'metrics.spikex' }
        ]
    },
    {
        chain: 'system-metrics-cpu',
        filters: [
            {
                filter: 'Metrics',
                config: {
                    update-interval: 15s,
                    metric-selector: 'system.cpu',
                    add-tags: [ 'metric', 'spikex-metric-jvm', 'oshi' ]
                }
            },
            { '%OutputAddress': 'metrics.spikex' }
        ]
    },
    {
        chain: 'system-metrics-memory',
        filters: [
            {
                filter: 'Metrics',
                config: {
                    update-interval: 15s,
                    metric-selector: 'system.memory',
                    add-tags: [ 'metric', 'spikex-metric-jvm', 'oshi' ]
                }
            },
            { '%OutputAddress': 'metrics.spikex' }
        ]
    },
    {
        chain: 'system-metrics-swap',
        filters: [
            {
                filter: 'Metrics',
                config: {
                    update-interval: 60s,
                    metric-selector: 'system.swap',
                    add-tags: [ 'metric', 'spikex-metric-jvm', 'oshi' ]
                }
            },
            { '%OutputAddress': 'metrics.spikex' }
        ]
    },
    {
        chain: 'system-metrics-filesystem',
        filters: [
            {
                filter: 'Metrics',
                config: {
                    update-interval: 60s,
                    metric-selector: 'filesystem',
                    add-tags: [ 'metric', 'spikex-metric-jvm', 'oshi' ]
                }
            },
            { '%OutputAddress': 'metrics.spikex' }
        ]
    },
    {
        chain: 'jvm-metrics-spikex',
        filters: [
            {
                filter: 'Metrics',
                config: {
                    update-interval: 15s,
                    metric-selector: 'jvm',
                    dsname-prefix: 'spikex',
                    add-tags: [ 'metric', 'spikex-metric-jvm', 'oshi' ]
                }
            },
            { '%OutputAddress': 'metrics.spikex' }
        ]
    },
    {
        chain: 'output-log',
        filters: [
            { '%InputAddress': 'metrics.spikex' },
            { filter: 'Log.out', config: { mdc-value: '%{@source}' } }
        ]
    }
]
----

Start Spike.x from a terminal or the command prompt and verify that no exceptions are thrown.

.Linux or OS X terminal
[source,shell]
cd /var/lib/spikex
su -c "./bin/spikex" spikex

.Windows command prompt as Administrator (assumes that Spike.x was installed on C:)
[source,shell]
cd \Program Files\Spike.x\spikex
bin\spikex.bat

Wait until you see from the output that Spike.x has successfully deployed the `io.spikex.filter.input.Metrics` 
and `io.spikex.filter.output.Logback` verticles.

You can then let Spike.x run for a while and finally stop it using `Ctrl+d` or `Ctrl+c` on Windows. 
Inspect the `events.log` file that's stored in the `log` directory. It should contain lines like these:

[source]
----
2015-12-06 12:37:50,384 Metrics {"@id":"65f2ab00-9c05-11e5-a3de-080027fe9e8b","@source":"Metrics","@timestamp":1449398270384,"@timezone":"UTC","@type":"metric","@chain":"system-metrics-memory","@priority":"normal","@host":"win7","@dsname":"system.memory","@dstype":"GAUGE","@dsprecision":"s","@subgroup":"used_perc","@instance":"-","@interval":15000,"@value":26.43424368265135,"@tags":["metric","spikex-metric-jvm","oshi"]}
2015-12-06 12:37:50,384 Metrics {"@id":"65f2ab01-9c05-11e5-a3de-080027fe9e8b","@source":"Metrics","@timestamp":1449398270384,"@timezone":"UTC","@type":"metric","@chain":"system-metrics-cpu","@priority":"normal","@host":"win7","@dsname":"system.cpu","@dstype":"GAUGE","@dsprecision":"s","@subgroup":"load_avg","@instance":"cpu1","@interval":15000,"@value":0.010510910058541777,"@tags":["metric","spikex-metric-jvm","oshi"]}
----

Please see the <<Troubleshooting>> section if you encounter problems. Usually the `spikex.log` contains the cause of the problem.

== Modules

Spike.x provides the following standard modules that are always available.

.Standard modules
[width="100%",options="header"]
|=======================================================================================
| Module                         | Description
| Core (mod-spikex-core)         | Base classes and services
| Filtering (mod-spikex-filter)  | Data input, filtering and output
| Metrics (mod-spikex-metrics)   | Spike.x JVM and system resource information (CPU, memory, disk, JVM heap, etc.)
| Notifier (mod-spikex-notifier) | Storing and sending of notifications
|=======================================================================================

[[module-core]]
=== Core

Technically core is not deployed as a Vert.x module. It's a library that's automatically available to all other modules.

Core contains the `Main` class of Spike.x that is used to bootstrap the Vert.x container. 

The `spikex.home` Java system property should be defined when starting Spike.x.
The property value is also used for the `vertx.home` property required by the Vert.x container.

The following Java system properties are used by Spike.x during bootstrap:

[width="100%",options="header"]
|=======================================================================================
| Property                    | Description                                       | Default value
| spikex.home                 | The home driectory of Spike.x                     | <none>
| spikex.user                 | The process user name                             | spikex
| spikex.pidfile              | The process PID file                              | /var/run/spikex.pid
| spikex.module.deploy.secs   | How long to wait (secs) for all modules to be deployed   | 45
| spikex.module.undeploy.secs | How long to wait (secs) for all modules to be undeployed | 45
|=======================================================================================

The `Main` class supports the following command line arguments:

[width="100%",options="header"]
|=======================================================================================
| Option           | Description                                   | Default value
| -conf            | Directory of configuration files              | vertx.home/conf
| -data            | Directory of data files                       | vertx.home/data
| -tmp             | Directory of temporary files                  | vertx.home/tmp
| -umask           | Umask used when creating new files            | 022 
| -daemon          | Start Spike.x as a daemon                     | false
| -version         | Output Spike.x version information and exit   | <none>
|=======================================================================================

The log directory is defined by the `logback.logdir` Java system property.

==== Configuration

The `spikex.yaml` is the main configuration file of Spike.x. It simply lists the modules that should be deployed.

.spikex.yaml
[source]
----
modules:
    - { id: io.spikex~spikex-filter~0.9.2 }
    - { id: io.spikex~spikex-metrics~0.9.2, config: { update-interval: 15000 }}
----

The `update-interval` option defines how often, in milliseconds, the metrics module should update its state.

[[module-filter]]
=== Filter

The filter module provides all the standard filters that enable Spike.x to perform streaming of events. 
Events can be received using input filters, manipulated using filters that take an input and an output and finally stored in some backend using output filters.

Filters and connected together using chains. One chain can contain one or more filters. Filters communicate with each other using `addresses`. 
Filters can have an input address, an output address or both. Events flow from one filter to the next one in a chain.

There are two special built-in filters that affect the input or output address of a filter:

[width="100%",options="header"]
|=======================================================================================
| Special filter   | Description
| %InputAddress    | Sets the input address of the next filter
| %OutputAddress   | Sets the output address of the previous filter
|=======================================================================================

These special filters enable us to define complex event routing between standard filters. 
A typical case is when you want to have multiple input chains that all output to the same backend. 
You can then define one filter configuration file per input chain and have one configuration file for the output chain.

==== Event

Events produces by filters contain some mandatory fields. 
These fields must always be available in events generated by any standard input filter:

[width="100%",options="header"]
|=======================================================================================
| Event field      | Description
| @id              | The UUID of the event
| @host            | The host identifier - free form string (short string)
| @source          | The source of the event - free form string (short string)
| @timestamp       | Event timestamp - the number of milliseconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970, not counting leap seconds.
| @timezone        | Event timezone - by default UTC
| @type            | The type of the event - must be one of the predefined event types: metric, notification, batch
| @tags            | The event tags - free form list of strings
| @chain           | The event chain - chain name of event
| @priority        | The event priority or severity - must be on of the pre-defined priorities: low, normal, high
|=======================================================================================

In addition metric events usually contain these fields:

[width="100%",options="header"]
|=======================================================================================
| Event field      | Description
| @value           | The metric event value
| @dsname          | The metric event datasource name
| @dstype          | The metric event datasource type - must be one of the pre-defined types: GAUGE, COUNTER, STRING
| @dsprecision     | The metric event datasource timestamp precision - must be one of the pre-defined types: n, u, ms, s, m, h
| @interval        | The metric event sampling interval in milliseconds - use "-" if not applicable
| @instance        | The metric event data instance - use "-" if not applicable
| @subgroup        | The metric event data subgroup - use "-" if not applicable
|=======================================================================================

These are mandatory fields for notification events:

[width="100%",options="header"]
|=======================================================================================
| Event field      | Description
| @title           | The notification event title or subject - free form string
| @message         | The notification event message - free form string
| @destinations    | The notification event destinations - list of destination addresses
|=======================================================================================

==== Configuration

The filter module supports one or more configuration files that start with the `filters` prefix. 
There are many example files in the `conf/examples` directory that you can use as such or with some modifications.

CAUTION: The filter module tries to automatically reload a modified configuration file. This might not always succeed. 
Please verify that the modifications were successfully loaded from the `spikex.log` file.

Many filters support the `update-interval` configuration option. It defines the interval of a specific filter operation. 
This can be given as an integer of milliseconds or using the following shorthand notation:

[width="100%",options="header"]
|=======================================================================================
| Notation         | Meaning
| <integer>s       | Interval in seconds. Example: '10s'
| <integer>m       | Interval in minutes. Example: '5m'
| <integer>h       | Interval in hours. Example: '1h'
|=======================================================================================

[[module-metrics]]
=== Metrics

The metrics module publishes resource data about the Spike.x JVM and the operating system. 
This can be useful if you want to monitor Spike.x resource usage.

==== Configuration

There is no configuration file for the metrics module. 
Instead you use the `Metrics` filter to grab the published Spike.x JVM metrics like this:

[source]
----
filter: 'Metrics',
config: {
    update-interval: '60s',
    metric-selector: 'system.load',
    add-tags: [ 'metric', 'spikex-metric-oshi', 'oshi' ]
}
----

Please see the `filters-metrics-oshi.yaml` example file for details.

[[module-notifier]]
=== Notifier

The notifier module can be used to publish notifications to interested parties. 
Currently this module supports sending of notifications to files, email addresses or to flowdock channels.
It supports templates, schedules, destinations and rules.

By default the notifier module listens to the `spikex.notifier` address for events.

==== Configuration

The notifier module reads the `notifier.yaml` configuration file.

CAUTION: The notifier module tries to automatically reload a modified configuration file. This might not always succeed. 
Please verify that the modifications were successfully loaded from the `spikex.log` file.

== Filters

In this section we present the configuration options for the standard filters.

=== Batch

Group events into a batch. The batch is contained in-memory and is not persisted during restarts.
The batch event contains three fields in addition to the mandatory fields: a type field, the amount of events it contains and a list of events.

==== Configuration

.Batch filter options
[width="100%",options="header"]
|===================================================================================================================
| Option                      | Description                                                                      | Default
| update-interval             | How often to publish the current batch if max batch size has not been reached    | <none>
| max-batch-size              | How many events to capture before publishing the batch                           | 1000
|===================================================================================================================

.Batch example
[source]
----
filter: 'Batch', 
config: { 
    update-interval: 500,
    max-batch-size: 100
}
----

=== Command

Execute a command. Command supports two modes: input and output. In input mode it executes a command and reads the 
command output line-by-line using a DSV line parser. In output mode it passes an event to the standard input of the command.

==== Configuration

.Command filter options
[width="100%",options="header"]
|===================================================================================================================
| Option                      | Description                                                                      | Default
| update-interval             | How often to execute the defined command                                         | <none>
| command                     | The command to execute                                                           | <none>
| args                        | The list of command arguments                                                    | <none>
| timeout                     | How long in milliseconds to wait for the command to exit                         | 2500
| encoding                    | What character encoding to use when parsing command output                       | UTF-8
| max-line-count              | Maximum amount of lines to read from the command output                          | 4000
| skip-lines-start            | How many lines to skip from the start of the command output before parsing lines | 0
| skip-lines-end              | How many lines to skip from the end of the command output                        | 0
| output-format               | The output format definition                                                     | <none>
| type                        | The output format type                                                           | dsv
|===================================================================================================================

.Command input example
[source]
----
filter: 'Command', 
config: { 
    update-interval: 5000,
    command: '/bin/cat',
    args: [ '/tmp/file.txt' ],
    output-format: {
        type: 'dsv'
    }
}
----

.Command output example
[source]
----
filter: 'Command', 
config: { 
    command: '/bin/cat',
    args: [ '/tmp/events.json' ]
}
----

=== Grok
==== Configuration

.Grok example
[source]
----
filter: 'Grok', 
config: { 
        patterns: [
            'file:%{#spikex.conf}/grok/base.grok',
            'file:%{#spikex.conf}/grok/log.grok'
        ],
        input-field: '@message',
        output-field: '@message',
        group: {
            fields: ['class', 'method', 'message', 'thread', 'level',
                       'year', 'month', 'day', 'hour', 'minute', 'second'],
            output-field: '@fields'
        },
        match-lines: [
           {
            pattern: '%{JAVAJBOSS4LOG:line}',
            tags: ['log', 'java'],
            ignore: ['JAVALVLCLS']
           }
        ],
        multi-line: {
            pattern: '%{JAVAEXCEPTION:line}',
            tags: ['error', 'exception'],
            segment-field: 'class'
        }
    }
}
----

=== Limit

Limit the amount of events per time unit if a rule is matched. A rule defines 
which events to match, when to match and the event rate limit.

==== Configuration

.Limit filter options
[width="100%",options="header"]
|===================================================================================================================
| Option                      | Description                                                                      | Default
| database-name               | The name of the limit state database                                             | limit.db
| database-compact-on-startup | Flag that controls database compaction on startup                                | true
| discard-on-mismatch         | Discard event if rule does not match                                             | true
| schedules                   | The schedules when the rule is active                                            | <none>
| throttles                   | The rate limiting definitions                                                    | <none>
| rules                       | The event matching and throttle rules                                            | <none>
|===================================================================================================================

===== Schedule
===== Throttle
===== Rule

.Limit example
[source]
----
filter: 'Limit', 
config: { 
    database-name: 'limit.db',
    database-compact-on-startup: true,
    discard-on-mismatch: false,
    schedules: { anytime: '* * * * *' },
    throttles: {
        one-per-15-minutes: {
            rate: 1,
            interval: 15,
            unit: 'min'
        }
    },
    rules: [
        {
            match-tag: 'ALARM',
            schedule: 'anytime',
            throttle: 'one-per-15-minutes'
        }
    ] 
}
----

=== Mutate

Modifies an event by adding or removing fields or tags. Also supports renaming of fields.
The modification can be done to every event or to specific events based on a rule.

==== Configuration

.Mutate filter options
[width="100%",options="header"]
|===================================================================================================================
| Option                      | Description                                                                      | Default
| add-fields                  | Map of fields to add to any event                                                | <none>
| add-tags                    | List of tags to add to any event                                                 | <none>
| del-fields                  | Map of fields to remove from any event                                           | <none>
| del-tags                    | List of tags to remove from any event                                            | <none>
| renames                     | Map of renaming definitions                                                      | <none>
| modifiers                   | The modifier definitions                                                         | <none>
| rules                       | The event matching rules                                                         | <none>
|===================================================================================================================

===== Rename
===== Modifier
===== Rule

.Mutate example
[source]
----
filter: 'Mutate', 
config: { 
    add-fields: { 
        cpu.total: '%{#metric.system.cpu.total.time}',
        io.total: '%{#metric.system.cpu.total.time}',
        mem.free: '%{#metric.system.memory.free.perc}',
        mem.jvm.use: '%{#metric.jvm.memory.perc}',
        '@type': 'spikex-process-metrics'
    },
    add-tags: [ 'system', 'load', 'cpu', 'mem', 'metric' ],
    modifiers: {
        low-resource: {
            add-tags: [ 'ALARM' ],
            add-fields: { '@alarm': 'low resource' }
        },
        no-files-alarm: {
            add-tags: [ 'ALARM' ],
            add-fields: { '@alarm': 'no files in directory for last 5 min' }
        }
    },
    rules: [
        { 
            match-field: 'mem.free',
            value-lte: 10,
            modifier: 'low-resource'
        },
        {
            match-field: 'cpu.total',
            value-gt: 95,
            modifier: 'low-resource'
        },
        {
            match-tag: 'dir-watcher',
            match-field: 'dir-name',
            value-in: [ 'xml-in', 'json-in' ],
            variables: {
                        'count': '%{file-count}',
                        'tm': '%{latest-timestamp}'
                    },
            # No files for 5 mins
            expression: 'if(count = 0 && #now(-5m) > tm)',
            modifier: 'no-files-alarm'
        }
    ]
}
----

=== Tail
==== Configuration
=== Http
==== Configuration
=== Elasticsearch
==== Configuration
=== Influxdb
==== Configuration
=== NSQ
==== Configuration
=== Logback
==== Configuration

== Out-of-box use cases

[[use-case-influxdb-grafana]]
=== Using InfluxDB and Grafana for monitoring

This use case describes how to monitor system metrics of a Windows host.

Start by setting up a https://customers.influxdb.com[hosted InfluxDB service] provided by https://influxdata.com[influxdata]. You can use the `Start Trial Now` option to test this use case.

Once you have access to the https://customers.influxdb.com[hosted service] you can <<Spike.x on Windows,install Spike.x on a Windows host>>.

Start by copying the `filters-metrics-oshi.yaml` and `filters-metrics-influxdb.yaml` example configuration files to the `conf` directory. 

Set the InfluxDB host, admin account and enable SSL in the `filters-metrics-influxdb.yaml` file:

[source,shell]
----
...
nodes: [ 'https://influxdb:e08b1f004b35ddae@pepsifree-gigawatt-16.c.influxdb.com:8086' ],
admin-user: 'influxdb',
admin-password: 'e08b1f004b35ddae',
ssl-enabled: true,
...
----

No modifications are needed to the `filters-metrics-oshi.yaml` configuration file. 

Test that the setup works by starting Spike.x from the command line using Administrator permissions.

[source,shell]
cd \Program Files\Spike.x\spikex
bin\spikex.bat

Please see the <<Troubleshooting>> section if you encounter problems. 
Otherwise login to the hosted Grafana service and setup the Spike.x datasource by simply entering `spikex` in the `Database` field and saving the value.

.Grafana Spike.x datasource
image:influxdb-hosted-grafana-datasource.png[]

You can now download and install https://www.nsclient.org[NSClient&#43;&#43;]. NSClient&#43;&#43; can be used to send system metrics to Spike.x on Windows.
Replace the default `nsclient.ini` with the one found in the `examples` directory of Spike.x. Also copy the `filters-metrics-nagios-nrdp.yaml` example configuration file to the `conf` directory.
Restart the `NSClient++` windows service and verify from the `events.log` file that you are receiving measurements.

That's it. Now you can continue by creating beautiful dashboards in Grafana.

[[use-case-elasticsearch-kibana]]
=== Using Elasticsearch and Kibana for event analysis

This use case describes how to analyze log events using https://www.elastic.co/products/elasticsearch[Elastichsearch] and https://www.elastic.co/products/kibana[Kibana].
This can be seen as a form of log shipping where we pre-filter the events to only contain data of interest.

Start by setting up a https://www.elastic.co/found[hosted Elasticsearch service] provided by https://www.elastic.co[Elastic]. You can use the `Free Trial` option to test this use case.

Once you have access to the https://www.elastic.co/found[hosted service] you can <<Installation,install Spike.x>> on a host that contains the log file to analyze.

Next we'll configure Spike.x to send some data to Elasticsearch to verify that the hosted service is working. 

[[use-case-notifications]]
=== Monitoring and sending of notifications

* CUPS queue monitoring in Linux
* Process/Service monitoring in Windows

[[use-case-influxdb-datapoint-replication]]
=== InfluxDB data point replication

[[use-case-log-tailing-spikex]]
=== Tailing logs using Spike.x

[[use-case-log-tailing-nxlog]]
=== Tailing logs using NXlog

== Troubleshooting

== Building Spike.x

Download or clone the sources from https://github.com/clidev/spike.x.git[GitHub].

[source,shell]
git clone https://github.com/clidev/spike.x.git
cd spike.x

List the available gradle projects with `gradlew projects`.

[source,shell]
 ./gradlew projects

List the available gradle tasks with `gradlew tasks`.

[source,shell]
 ./gradlew tasks

Compile, test and build Spike.x with `gradlew clean build`.

[source,shell]
 ./gradlew clean build

Create the installation packages with `gradlew izpack launch4j`. The `launch4j` 
task works in Linux, Windows and OS X.

[source,shell]
 ./gradlew izpack launch4j


CAUTION: Spike.x 0.9.x and Gradle 1.10 does not support Maven profiles. When
resolving dependencies you get the following kind of errors:

[source]
----
FAILURE: Build failed with an exception.

* What went wrong:
Could not resolve all dependencies for configuration ':spikex-core:provided'.
> Could not resolve ch.qos.logback:logback-classic:1.1.1.
  Required by:
      io.spikex:spikex-core:0.9.0
   > Could not parse POM http://jcenter.bintray.com/ch/qos/logback/logback-classic/1.1.1/logback-classic-1.1.1.pom
      > Resetting to invalid mark
   > Could not parse POM http://repo1.maven.org/maven2/ch/qos/logback/logback-classic/1.1.1/logback-classic-1.1.1.pom
      > Resetting to invalid mark
   > Could not parse POM http://repo1.maven.org/maven2/ch/qos/logback/logback-classic/1.1.1/logback-classic-1.1.1.pom
      > Resetting to invalid mark
> Could not resolve org.kohsuke:akuma:1.9.
  Required by:
      io.spikex:spikex-core:0.9.0
   > Could not parse POM http://jcenter.bintray.com/org/kohsuke/akuma/1.9/akuma-1.9.pom
      > Content is not allowed in prolog.
   > Could not parse POM http://repo1.maven.org/maven2/org/kohsuke/akuma/1.9/akuma-1.9.pom
      > Content is not allowed in prolog.
   > Could not parse POM http://repo1.maven.org/maven2/org/kohsuke/akuma/1.9/akuma-1.9.pom
      > Content is not allowed in prolog.
----

---
Spike.x - Sense. React. Visualize.
